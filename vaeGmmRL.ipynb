{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import gym\n",
    "from gym import spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========================== 1. VAE Definition ============================= #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Variational Autoencoder for compressing stock data into a latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=5, latent_dim=2):\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2_mu = nn.Linear(16, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(16, latent_dim)\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, 16)\n",
    "        self.fc4 = nn.Linear(16, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc2_mu(h)\n",
    "        logvar = self.fc2_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc3(z))\n",
    "        return self.fc4(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(reconstructed, original, mu, logvar):\n",
    "    mse = nn.MSELoss(reduction=\"sum\")\n",
    "    recon_loss = mse(reconstructed, original)\n",
    "    # KL Divergence\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld\n",
    "\n",
    "def train_vae(model, data, epochs=10, batch_size=64, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train the VAE using the provided data (numpy).\n",
    "    \"\"\"\n",
    "    dataset = TensorDataset(torch.tensor(data, dtype=torch.float32))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            batch_features = batch[0]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, mu, logvar = model(batch_features)\n",
    "            loss = vae_loss_function(reconstructed, batch_features, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] - VAE Loss: {total_loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============ 2. (Optional) GMM for Market Regime Clustering =============== #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gmm(latent_features, n_components=3):\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    gmm.fit(latent_features)\n",
    "    return gmm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "    Normalize and preprocess a dictionary-based dataset for training a VAE:\n",
    "    - Normalizes numerical data per stock\n",
    "    - Converts to a single NumPy array for batch training\n",
    "    - Stores individual scalers per stock\n",
    "    \n",
    "    Returns:\n",
    "        - `processed_data` (np.ndarray): Concatenated dataset (all stocks)\n",
    "        - `scaler_dict` (dict): Dictionary of scalers for each stock\n",
    "        - `stock_labels` (np.ndarray): Stock identifiers for each row\n",
    "    \"\"\"\n",
    "    scaler_dict = {}  # Stores scalers per stock\n",
    "    all_features = []  # Stores all preprocessed features\n",
    "    stock_labels = []  # Stock identifiers\n",
    "\n",
    "    for stock_symbol, df in dataset.dfs.items():\n",
    "        if df.empty:\n",
    "            continue  # Skip empty DataFrames\n",
    "\n",
    "        # Convert DataFrame to NumPy array\n",
    "        features = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].values\n",
    "\n",
    "        # Normalize using StandardScaler per stock\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "        # Append to all_features list\n",
    "        all_features.append(scaled_features)\n",
    "\n",
    "        # Keep track of which stock each row belongs to\n",
    "        stock_labels.append(np.full(len(scaled_features), stock_symbol))\n",
    "\n",
    "        # Store the scaler for inverse transform later\n",
    "        scaler_dict[stock_symbol] = scaler\n",
    "\n",
    "    # Stack all stock data into a single NumPy array\n",
    "    processed_data = np.vstack(all_features)  # Shape: (total_rows, 5)\n",
    "    stock_labels = np.concatenate(stock_labels)  # Shape: (total_rows,)\n",
    "\n",
    "    return processed_data, scaler_dict, stock_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockTradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A simplified stock trading environment where the observation is:\n",
    "      - The latent representation from the VAE (and optionally GMM cluster info)\n",
    "      - The current (or recent) price\n",
    "\n",
    "    The agent takes one of three actions:\n",
    "      0 = HOLD, 1 = BUY (go long), 2 = SELL (liquidate or short)\n",
    "\n",
    "    Reward is based on changes in portfolio value over time.\n",
    "    This environment simulates a single share approach for demonstration.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        price_history, \n",
    "        latent_history, \n",
    "        initial_balance=10000.0,\n",
    "        max_shares=10\n",
    "    ):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "\n",
    "        self.price_history = price_history  # shape [T,]\n",
    "        self.latent_history = latent_history  # shape [T, latent_dim or more]\n",
    "        self.n_steps = len(price_history)\n",
    "\n",
    "        self.initial_balance = initial_balance\n",
    "        self.max_shares = max_shares\n",
    "\n",
    "        # Define action & observation space\n",
    "        # Actions: 0=Hold, 1=Buy, 2=Sell\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observations: latent dim + (current price, shares held, balance ratio)\n",
    "        # - You can customize this as needed\n",
    "        obs_dim = self.latent_history.shape[1] + 3\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # Internal states\n",
    "        self.reset()\n",
    "\n",
    "    def _get_observation(self):\n",
    "        \"\"\"Construct observation from latent vector, price, shares held, balance ratio.\"\"\"\n",
    "        latent = self.latent_history[self.current_step]\n",
    "        current_price = self.price_history[self.current_step]\n",
    "        balance_ratio = self.cash / self.initial_balance\n",
    "        obs = np.concatenate([\n",
    "            latent, \n",
    "            [current_price, float(self.shares_held), balance_ratio]\n",
    "        ])\n",
    "        return obs\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take an action (buy, sell, hold) and compute reward.\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        current_price = self.price_history[self.current_step]\n",
    "\n",
    "        # Execute action\n",
    "        if action == 1:  # BUY 1 share (if possible)\n",
    "            if self.cash >= current_price and self.shares_held < self.max_shares:\n",
    "                self.shares_held += 1\n",
    "                self.cash -= current_price\n",
    "        elif action == 2:  # SELL all shares (for simplicity)\n",
    "            if self.shares_held > 0:\n",
    "                self.cash += self.shares_held * current_price\n",
    "                self.shares_held = 0\n",
    "        # If action == 0 (HOLD), do nothing\n",
    "\n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= self.n_steps - 1:\n",
    "            done = True\n",
    "\n",
    "        # Calculate reward as the net worth change from previous step\n",
    "        net_worth = self.cash + self.shares_held * current_price\n",
    "        reward = net_worth - self.prev_net_worth\n",
    "        self.prev_net_worth = net_worth\n",
    "\n",
    "        obs = self._get_observation()\n",
    "        info = {\"net_worth\": net_worth}\n",
    "        return obs, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.prev_net_worth = self.initial_balance\n",
    "        return self._get_observation()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleQNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal DQN-like network that outputs Q-values for each action.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_actions=3):\n",
    "        super(SimpleQNetwork, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_agent(env, q_network, episodes=10, gamma=0.95, lr=1e-3, epsilon=1.0, epsilon_decay=0.95):\n",
    "    \"\"\"\n",
    "    A toy training loop for a DQN-style agent. This uses a very simple\n",
    "    on-policy approach (no replay buffer, no target network, etc.).\n",
    "    In a real system, use a well-tested RL library like Stable-Baselines3.\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        done = False\n",
    "        episode_reward = 0.0\n",
    "\n",
    "        while not done:\n",
    "            # Epsilon-greedy action\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values = q_network(state_t)\n",
    "                action = q_values.argmax().item()\n",
    "\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            next_state_t = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            # Compute target\n",
    "            with torch.no_grad():\n",
    "                next_q_values = q_network(next_state_t)\n",
    "            max_next_q = next_q_values.max(1)[0].item()\n",
    "            target_q = reward + (gamma * max_next_q if not done else 0.0)\n",
    "\n",
    "            # Current Q\n",
    "            current_q = q_network(state_t)[0, action]\n",
    "\n",
    "            # Update Q-network\n",
    "            loss = loss_fn(current_q, torch.tensor(target_q, dtype=torch.float32))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Move to next step\n",
    "            state_t = next_state_t\n",
    "            episode_reward += reward\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(0.01, epsilon * epsilon_decay)\n",
    "        print(f\"Episode {ep+1}/{episodes} - Reward: {episode_reward:.2f}, Epsilon: {epsilon:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # ---------------- Step A: Download Data  ----------------- #\n",
    "    file = \"csvs/NVDA.csv\"\n",
    "    df = pd.read_csv(file, parse_dates=['Datetime'], index_col='Datetime')\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.sort_index()  # ensure chronological order\n",
    "\n",
    "    # For simplicity, we'll just use [Open, High, Low, Close, Volume]\n",
    "    features = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].values\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # --------------- Step B: Train VAE on Historical Data ----------------#\n",
    "    vae = VAE(input_dim=5, latent_dim=2)\n",
    "    train_vae(vae, scaled_features, epochs=5, batch_size=32, lr=1e-3)\n",
    "\n",
    "    # Generate latent vectors for each time step\n",
    "    vae.eval()\n",
    "    latent_vectors = []\n",
    "    with torch.no_grad():\n",
    "        for row in scaled_features:\n",
    "            x = torch.tensor(row, dtype=torch.float32).unsqueeze(0)\n",
    "            mu, logvar = vae.encode(x)\n",
    "            z = vae.reparameterize(mu, logvar)\n",
    "            latent_vectors.append(z.numpy().flatten())\n",
    "    latent_vectors = np.array(latent_vectors)\n",
    "    # =========================== 5. Putting It All Together ====================== #\n",
    "\n",
    "\n",
    "    # ----- (Optional) Train GMM if you want cluster-based features ------- #\n",
    "    # Here we skip it or do it quickly:\n",
    "    # gmm = train_gmm(latent_vectors, n_components=3)\n",
    "    # cluster_probs = gmm.predict_proba(latent_vectors)\n",
    "    # or cluster_labels = gmm.predict(latent_vectors)\n",
    "        # -------------- Step C: Prepare Gym Environment for RL ---------------#\n",
    "    close_prices = df[\"Close\"].values\n",
    "    # For simplicity, let's directly use 'latent_vectors' as our RL observation input.\n",
    "    # If you want to incorporate GMM, you'd merge cluster_probs into the observation.\n",
    "\n",
    "    env = StockTradingEnv(price_history=close_prices, latent_history=latent_vectors)\n",
    "\n",
    "    # -------------- Step D: Train a Simple DQN-like Agent ----------------#\n",
    "    observation_dim = env.observation_space.shape[0]\n",
    "    q_net = SimpleQNetwork(input_dim=observation_dim, hidden_dim=64, num_actions=3)\n",
    "    train_rl_agent(env, q_net, episodes=10, gamma=0.95, lr=1e-3)\n",
    "\n",
    "    # -------------- Step E: Evaluate the Trained Agent (Backtesting) -----#\n",
    "    # We'll do a single run through the environment to see final performance\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done:\n",
    "        state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = q_net(state_t)\n",
    "        action = q_values.argmax().item()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "    print(f\"Final net worth: {info['net_worth']:.2f}\")\n",
    "    print(f\"Total reward earned in final run: {total_reward:.2f}\")\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5] - VAE Loss: 1050629.65\n",
      "Epoch [2/5] - VAE Loss: 89029.20\n",
      "Epoch [3/5] - VAE Loss: 51242.16\n",
      "Epoch [4/5] - VAE Loss: 42911.81\n",
      "Epoch [5/5] - VAE Loss: 39359.01\n",
      "Episode 1/10 - Reward: 25.86, Epsilon: 0.95\n",
      "Episode 2/10 - Reward: -50.19, Epsilon: 0.90\n",
      "Episode 3/10 - Reward: -33.54, Epsilon: 0.86\n",
      "Episode 4/10 - Reward: -16.54, Epsilon: 0.81\n",
      "Episode 5/10 - Reward: -55.10, Epsilon: 0.77\n",
      "Episode 6/10 - Reward: 35.90, Epsilon: 0.74\n",
      "Episode 7/10 - Reward: -1.52, Epsilon: 0.70\n",
      "Episode 8/10 - Reward: -7.43, Epsilon: 0.66\n",
      "Episode 9/10 - Reward: -67.77, Epsilon: 0.63\n",
      "Episode 10/10 - Reward: 30.90, Epsilon: 0.60\n",
      "Final net worth: 10000.00\n",
      "Total reward earned in final run: 0.00\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
