{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "import backtrader as bt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from gym import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import collections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MODEL_SAVE_PATH = \"dqn_trading_model.pth\"\n",
    "SP500_TICKERS = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "\n",
    "\n",
    "class StockDataset:\n",
    "    def __init__(self, tickers, num_segments=10):\n",
    "        self.tickers = tickers\n",
    "        self.num_segments = num_segments  # Number of parts to divide the dataset into\n",
    "        self.segmented_data = []  # Preprocessed data\n",
    "        self.current_ticker_idx = 0  # Keep track of which stock is being processed\n",
    "        \n",
    "        self._load_and_split_data()\n",
    "    \n",
    "    def _load_and_split_data(self):\n",
    "        \"\"\"Loads full dataset once and splits into shuffled segments\"\"\"\n",
    "        for ticker in self.tickers:\n",
    "            stock_data = yf.download(ticker, period='max', progress=False)\n",
    "            if stock_data.empty:\n",
    "                continue\n",
    "            \n",
    "            values = stock_data[['Open', 'High', 'Low', 'Close', 'Volume']].dropna().to_numpy(dtype=np.float32)\n",
    "            segment_size = len(values) // self.num_segments\n",
    "\n",
    "            # Split dataset into segments\n",
    "            segments = [values[i * segment_size:(i + 1) * segment_size] for i in range(self.num_segments)]\n",
    "            self.segmented_data.extend([(segment, ticker) for segment in segments])\n",
    "        \n",
    "        # Shuffle segments to ensure model trains on different periods randomly\n",
    "        random.shuffle(self.segmented_data)\n",
    "\n",
    "    def fetch_next_stock(self):\n",
    "        \"\"\"Returns a dataset segment in order, keeping track of stock index.\"\"\"\n",
    "        if not self.segmented_data:\n",
    "            return None, None  # No more stocks\n",
    "        \n",
    "        if self.current_ticker_idx >= len(self.segmented_data):\n",
    "            self.current_ticker_idx = 0  # Loop back to start\n",
    "        \n",
    "        segment, ticker = self.segmented_data[self.current_ticker_idx]\n",
    "        self.current_ticker_idx += 1\n",
    "        return segment, ticker\n",
    "\n",
    "    def save(self, path):\n",
    "        \"\"\"\n",
    "        Saves the StockDataset to a pickle file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The path to the pickle file.\n",
    "        \"\"\"\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self, f)\n",
    "        print('Saved Dataset.')\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(path):\n",
    "        \"\"\"\n",
    "        Loads the StockDataset from a pickle file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): The path to the pickle file.\n",
    "\n",
    "        Returns:\n",
    "            StockDataset: The loaded StockDataset object.\n",
    "        \"\"\"\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: Preprocess Data\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocesses stock data by adding features and scaling.\"\"\"\n",
    "    df['Return'] = df['Close'].pct_change()\n",
    "    df['SMA150'] = df['Close'].rolling(window=150).mean()\n",
    "    df.dropna(inplace=True)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(df[['Open', 'Close', 'Return' 'Volume', 'SMA150']])\n",
    "    return scaled_data, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data, trading_mode='both', transaction_fee=5):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.trading_mode = trading_mode  # 'long', 'short', or 'both'\n",
    "        self.transaction_fee = transaction_fee\n",
    "        self.current_step = 0\n",
    "        self.cash = 10000\n",
    "        self.position = 0\n",
    "        self.short_position = 0\n",
    "        self.done = False\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)\n",
    "\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.cash = 10000\n",
    "        self.position = 0\n",
    "        self.done = False\n",
    "        return self._get_observation()\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Ensure proper extraction of scalar values to avoid deprecation warnings.\"\"\"\n",
    "        if self.current_step >= len(self.data):\n",
    "            self.current_step = len(self.data) - 1\n",
    "        return np.concatenate((self.data[self.current_step], np.array([self.position, self.cash], dtype=np.float32)))\n",
    "    \n",
    "    def step(self, action):\n",
    "        current_price = self.data[self.current_step][3]  # Close price\n",
    "\n",
    "        if action == 1 and self.trading_mode in ['long', 'both']:  # Buy\n",
    "            shares_to_buy = self.cash // current_price\n",
    "            self.cash -= shares_to_buy * current_price + self.transaction_fee\n",
    "            self.position += shares_to_buy\n",
    "\n",
    "        elif action == 2 and self.trading_mode in ['short', 'both']:  # Short sell\n",
    "            shares_to_short = self.cash // current_price\n",
    "            self.cash += shares_to_short * current_price - self.transaction_fee\n",
    "            self.short_position += shares_to_short\n",
    "\n",
    "        elif action == 2 and self.position > 0:  # Sell long position\n",
    "            self.cash += self.position * current_price - self.transaction_fee\n",
    "            self.position = 0\n",
    "\n",
    "        elif action == 1 and self.short_position > 0:  # Cover short position\n",
    "            self.cash -= self.short_position * current_price + self.transaction_fee\n",
    "            self.short_position = 0\n",
    "        \n",
    "        self.current_step += 1\n",
    "        reward = self.cash + (self.position - self.short_position) * self.data[self.current_step][3] - 10000\n",
    "\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_observation(), reward, self.done, {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(stock_data, model):\n",
    "    \"\"\"Evaluates the model on stock data and returns total reward.\"\"\"\n",
    "    env = TradingEnv(stock_data)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    actions = []\n",
    "    \n",
    "    while not done:\n",
    "        with torch.no_grad():\n",
    "            q_values = model(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "            action = torch.argmax(q_values).item()\n",
    "        \n",
    "        actions.append(action)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "    \n",
    "    return total_reward, actions\n",
    "\n",
    "# Function to plot evaluation results\n",
    "def plot_evaluation_results(stock_data, actions):\n",
    "    \"\"\"Plots stock data and overlays model evaluation results.\"\"\"\n",
    "    close_prices = stock_data[:, 3]  # Close prices\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(close_prices, label=\"Close Price\", color='black')\n",
    "    \n",
    "    buy_signals = [i for i in range(len(actions)) if actions[i] == 1]\n",
    "    sell_signals = [i for i in range(len(actions)) if actions[i] == 2]\n",
    "    \n",
    "    plt.scatter(buy_signals, close_prices[buy_signals], color='green', marker='^', label='Buy')\n",
    "    plt.scatter(sell_signals, close_prices[sell_signals], color='red', marker='v', label='Sell')\n",
    "    \n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Stock Price\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Model Evaluation Results\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_or_load(input_dim: int = 7,\n",
    "                     output_dim: int = 3,\n",
    "                     lr: float = 1e-3,\n",
    "                     memory_size: int = 10_000):\n",
    "    \"\"\"\n",
    "    Returns (model, optimizer, memory), loading pretrained weights if available.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    model = StockLSTM(input_dim=input_dim, output_dim=output_dim).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    memory = collections.deque(maxlen=memory_size)\n",
    "\n",
    "    if os.path.exists(MODEL_SAVE_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        print(f\"✅ Loaded existing model from {MODEL_SAVE_PATH}\")\n",
    "    else:\n",
    "        print(\"ℹ️  No existing model found — initialized new network.\")\n",
    "\n",
    "    return model, optimizer, memory, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if os.path.exists(\"data/stock_dataset.pkl\"):\n",
    "    dataset = StockDataset.load(\"data/stock_dataset.pkl\")\n",
    "else:\n",
    "    tickers = pd.read_html(SP500_TICKERS)[0]['Symbol'].tolist()\n",
    "    dataset = StockDataset(tickers, num_segments=10)\n",
    "    dataset.save(\"data/stock_dataset.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64, num_layers=3, output_dim=3):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_dim)\n",
    "        _, (h_n, _) = self.lstm(x)  # h_n: (num_layers, batch_size, hidden_dim)\n",
    "        last_hidden = h_n[-1]       # (batch_size, hidden_dim)\n",
    "        output = self.fc(last_hidden)  # (batch_size, output_dim)\n",
    "        return output\n",
    "\n",
    "# Train the RL agent using StockDataset\n",
    "def train_dqn_on_sp500(detaset, episodes=10, batch_size=64, gamma=0.95, lr=0.001, save_interval=9):\n",
    "    \n",
    "    model, optimizer, memory, device = init_or_load(input_dim=7, output_dim=3, lr=lr)\n",
    "\n",
    "    # Load existing model if available\n",
    "    if os.path.exists(MODEL_SAVE_PATH):\n",
    "        model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "        print(\"Loaded existing model.\")\n",
    "    \n",
    "    while True:  # Keep training as long as there are segments\n",
    "        \n",
    "        stock_data, ticker = dataset.fetch_next_stock()\n",
    "        if stock_data is None:\n",
    "            print(\"All data segments processed, restarting training loop...\")\n",
    "            break  # Exit training loop if all data is processed\n",
    "        \n",
    "        env = TradingEnv(stock_data)\n",
    "        wins = 0\n",
    "        \n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            PROFIT = 0\n",
    "            tax_credit = 0\n",
    "            \n",
    "            \n",
    "            while not done:\n",
    "                if random.random() < 0.1:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        q_values = model(torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0))\n",
    "                        action = torch.argmax(q_values).item()\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                memory.append((state, action, reward, next_state, done))\n",
    "                \n",
    "                if len(memory) > batch_size:\n",
    "                    batch = random.sample(memory, batch_size)\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                    \n",
    "                    states = torch.tensor(np.array(states), dtype=torch.float32, device=device)\n",
    "                    actions = torch.tensor(actions, dtype=torch.int64, device=device)\n",
    "                    rewards = torch.tensor(rewards, dtype=torch.float32, device=device)\n",
    "                    next_states = torch.tensor(np.array(next_states), dtype=torch.float32, device=device)\n",
    "                    dones = torch.tensor(dones, dtype=torch.float32, device=device)\n",
    "                    \n",
    "                    q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                    next_q_values = model(next_states).max(1)[0].detach()\n",
    "                    expected_q_values = rewards + gamma * next_q_values * (1 - dones)\n",
    "                    \n",
    "                    loss = nn.CrossEntropyLoss()(q_values, expected_q_values)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Assume tax_credit is a running float (starts at 0)\n",
    "            # PROFIT is your cumulative net profit\n",
    "\n",
    "            if total_reward > 0:\n",
    "                # Compute gross tax liability\n",
    "                tax_liability = 0.25 * total_reward\n",
    "                wins += 1\n",
    "\n",
    "                # Apply credit\n",
    "                tax_due = max(tax_liability - tax_credit, 0)\n",
    "\n",
    "                # Update remaining credit (if credit > liability)\n",
    "                tax_credit = max(tax_credit - tax_liability, 0)\n",
    "            else:\n",
    "                # Losses generate new credits (write‑offs)\n",
    "                tax_credit += abs(total_reward) * 0.25\n",
    "                tax_due = 0\n",
    "\n",
    "            # Subtract tax from profit\n",
    "            net_profit = total_reward - tax_due\n",
    "            PROFIT += net_profit\n",
    "            win_rate = wins / (episode + 1)  \n",
    "            print(f\"Ticker: {ticker} | Episode {episode+1}/{episodes} | Reward: {reward:.2f} |total reward: {total_reward:.2f} | wins: {win_rate}\" )\n",
    "            \n",
    "            if episode % save_interval == 0:\n",
    "                torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "                print(\"Model progress saved.\")\n",
    "        \n",
    "        # Evaluate model after training and plot results\n",
    "        \n",
    "        total_reward, actions = evaluate_model(stock_data, model)\n",
    "        plot_evaluation_results(stock_data, actions)\n",
    "    \n",
    "    print(\"Training complete.\")\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "    print(\"Final model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "ℹ️  No existing model found — initialized new network.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtrain_dqn_on_sp500\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 63\u001b[0m, in \u001b[0;36mtrain_dqn_on_sp500\u001b[1;34m(detaset, episodes, batch_size, gamma, lr, save_interval)\u001b[0m\n\u001b[0;32m     60\u001b[0m next_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39marray(next_states), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     61\u001b[0m dones \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(dones, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 63\u001b[0m q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     64\u001b[0m next_q_values \u001b[38;5;241m=\u001b[39m model(next_states)\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     65\u001b[0m expected_q_values \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m next_q_values \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m dones)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run training\n",
    "train_dqn_on_sp500(dataset, episodes=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
