{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from random import choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "=============================================================================\n",
    " VAE-GMM Stock Prediction Model\n",
    "-----------------------------------------------------------------------------\n",
    " This file implements an end-to-end pipeline for:\n",
    "   1. Data Ingestion & Preprocessing\n",
    "   2. Variational Autoencoder (VAE) Training\n",
    "   3. Gaussian Mixture Model (GMM) Clustering\n",
    "   4. Probability Prediction & Buy/Sell Signal Generation\n",
    "   5. Backtesting\n",
    "   6. (Optional) Real-time Inference Pipeline\n",
    "\n",
    " Reference: VAE-GMM Stock Prediction Model Specification Document\n",
    "=============================================================================\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ======================= Section 1: Data Ingestion ===========================\n",
    "# Ref. Specification 2.1: \"Data Ingestion: The system should collect OHLCV \n",
    "# price data, fundamentals, and macroeconomic indicators.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StockDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom PyTorch dataset to handle stock time series and fundamental data.\n",
    "    Reference: Specification 3.1 - Data Sources & 3.2 - Data Preprocessing\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, period, transform):\n",
    "\n",
    "        self.period = period #day/ csvs(minute)\n",
    "        self.dfs = {}\n",
    "        self.transform = transform\n",
    "        \n",
    "        \n",
    "        for name in ('IWM', 'NVDA', 'QQQ', 'SPY', 'TSLA'):\n",
    "\n",
    "            self.file = f\"{self.period}/{name}.csv\"\n",
    "            if not os.path.exists(self.file):  # ✅ Check if file exists before loading\n",
    "                print(f\"⚠️ Warning: File {self.file} not found. Skipping...\")\n",
    "                continue  # Skip this stock if file doesn't exist\n",
    "\n",
    "\n",
    "            # load\n",
    "            df = pd.read_csv(self.file, parse_dates=['Datetime'], index_col='Datetime')\n",
    "            df.index = pd.to_datetime(df.index, errors='coerce')\n",
    "            # Basic cleaning of missing values (forward fill)\n",
    "            df.ffill()\n",
    "            self.dfs.update({name : df})\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(df) for df in dfs)\n",
    "\n",
    "    def __getitem__(self, key_idx):\n",
    "        # Example features: [Open, High, Low, Close, Volume]\n",
    "        stock, idx = key_idx\n",
    "        if stock not in self.dfs:\n",
    "            raise KeyError(f\"Stock '{stock}' not found in dataset.\")\n",
    "\n",
    "        df = self.dfs[stock]  # Retrieve the correct DataFrame\n",
    "\n",
    "        if idx >= len(df):\n",
    "            raise IndexError(f\"Index {idx} out of bounds for '{stock}'.\")\n",
    "    \n",
    "        row = df.iloc[idx]\n",
    "        features = np.array([row[\"Open\"], row[\"High\"], row[\"Low\"],\n",
    "                             row[\"Close\"], row[\"Volume\"]], dtype=np.float32)\n",
    "        \n",
    "        # (Optional) apply transformations if needed\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ================== Section 2: Data Preprocessing & Utilities ===============\n",
    "# Ref. Specification 3.2: \"Data Preprocessing Steps: Normalize numerical data, \n",
    "# log-scale, handle missing, etc.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(dataset):\n",
    "    \"\"\"\n",
    "    Normalize and preprocess a dictionary-based dataset for training a VAE:\n",
    "    - Normalizes numerical data per stock\n",
    "    - Converts to a single NumPy array for batch training\n",
    "    - Stores individual scalers per stock\n",
    "    \n",
    "    Returns:\n",
    "        - `processed_data` (np.ndarray): Concatenated dataset (all stocks)\n",
    "        - `scaler_dict` (dict): Dictionary of scalers for each stock\n",
    "        - `stock_labels` (np.ndarray): Stock identifiers for each row\n",
    "    \"\"\"\n",
    "    scaler_dict = {}  # Stores scalers per stock\n",
    "    all_features = []  # Stores all preprocessed features\n",
    "    stock_labels = []  # Stock identifiers\n",
    "\n",
    "    for stock_symbol, df in dataset.dfs.items():\n",
    "        if df.empty:\n",
    "            continue  # Skip empty DataFrames\n",
    "\n",
    "        # Convert DataFrame to NumPy array\n",
    "        features = df[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]].values\n",
    "\n",
    "        # Normalize using StandardScaler per stock\n",
    "        scaler = StandardScaler()\n",
    "        scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "        # Append to all_features list\n",
    "        all_features.append(scaled_features)\n",
    "\n",
    "        # Keep track of which stock each row belongs to\n",
    "        stock_labels.append(np.full(len(scaled_features), stock_symbol))\n",
    "\n",
    "        # Store the scaler for inverse transform later\n",
    "        scaler_dict[stock_symbol] = scaler\n",
    "\n",
    "    # Stack all stock data into a single NumPy array\n",
    "    processed_data = np.vstack(all_features)  # Shape: (total_rows, 5)\n",
    "    stock_labels = np.concatenate(stock_labels)  # Shape: (total_rows,)\n",
    "\n",
    "    return processed_data, scaler_dict, stock_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ===================== Section 3: Variational Autoencoder ====================\n",
    "# Ref. Specification 4.1: \"VAE Model: Train a Variational Autoencoder (VAE)\n",
    "# to extract latent features.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple Variational Autoencoder for stock time series as described in\n",
    "    Specification 4.1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=5, latent_dim=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Number of input features (e.g., OHLCV).\n",
    "            latent_dim (int): Dimensionality of the latent space.\n",
    "        \"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        # Encoder\n",
    "        self.fc1 = nn.Linear(input_dim, 16)\n",
    "        self.fc2_mu = nn.Linear(16, latent_dim)\n",
    "        self.fc2_logvar = nn.Linear(16, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(latent_dim, 16)\n",
    "        self.fc4 = nn.Linear(16, input_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        mu = self.fc2_mu(h)\n",
    "        logvar = self.fc2_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = torch.relu(self.fc3(z))\n",
    "        return self.fc4(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed = self.decode(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "\n",
    "def vae_loss_function(reconstructed, original, mu, logvar):\n",
    "    \"\"\"\n",
    "    Combination of Reconstruction Loss (MSE) and KL Divergence.\n",
    "    Reference: Specification 4.1.\n",
    "    \"\"\"\n",
    "    mse = nn.MSELoss(reduction=\"sum\")\n",
    "    recon_loss = mse(reconstructed, original)\n",
    "\n",
    "    # KL Divergence\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld\n",
    "\n",
    "def train_vae(model, data, epochs=10, batch_size=64, lr=1e-3):\n",
    "    \"\"\"\n",
    "    Train a Variational Autoencoder (VAE) using the preprocessed stock dataset.\n",
    "\n",
    "    Args:\n",
    "        model (VAE): The VAE model.\n",
    "        data (np.ndarray): Preprocessed numerical stock data.\n",
    "        epochs (int): Number of training epochs.\n",
    "        batch_size (int): Batch size.\n",
    "        lr (float): Learning rate.\n",
    "    \"\"\"\n",
    "    # Convert data to PyTorch tensor\n",
    "    dataset = TensorDataset(torch.tensor(data, dtype=torch.float32))\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_features in dataloader:\n",
    "            batch_features = batch_features[0]  # Extract from dataset tuple\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, mu, logvar = model(batch_features)\n",
    "\n",
    "            loss = vae_loss_function(reconstructed, batch_features, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ==================== Section 4: Gaussian Mixture Model ======================\n",
    "# Ref. Specification 4.2: \"GMM Model: Train a Gaussian Mixture Model (GMM) \n",
    "# to cluster the latent space into different market regimes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_gmm(latent_features, n_components=3):\n",
    "    \"\"\"\n",
    "    Train a Gaussian Mixture Model on the latent features.\n",
    "    \n",
    "    Args:\n",
    "        latent_features (np.ndarray): Latent representations from the VAE.\n",
    "        n_components (int): Number of mixture components.\n",
    "        \n",
    "    Returns:\n",
    "        gmm (GaussianMixture): Trained GMM model.\n",
    "    \"\"\"\n",
    "    gmm = GaussianMixture(n_components=n_components, random_state=42)\n",
    "    gmm.fit(latent_features)\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ============= Section 5: Buy/Sell Decision Mechanism & Backtesting ==========\n",
    "# Ref. Specification 4.3: \"If Bullish Probability > 70% -> BUY, etc.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_signals(probabilities, bullish_index=0, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Generate buy/sell/hold signals based on probability distribution from GMM.\n",
    "\n",
    "    Args:\n",
    "        probabilities (np.ndarray): Probabilities for each GMM component.\n",
    "        bullish_index (int): Index of the GMM component considered \"bullish\".\n",
    "        threshold (float): Probability threshold for buy/sell signals.\n",
    "\n",
    "    Returns:\n",
    "        signals (list): List of signals (\"BUY\", \"SELL\", or \"HOLD\").\n",
    "    \"\"\"\n",
    "    signals = []\n",
    "    for prob in probabilities:\n",
    "        # Example: If cluster '0' is bullish, cluster '1' might be bearish, etc.\n",
    "        bullish_prob = prob[bullish_index]  # e.g., first cluster as bullish\n",
    "        if bullish_prob > threshold:\n",
    "            signals.append(\"BUY\")\n",
    "        elif bullish_prob < (1 - threshold):\n",
    "            signals.append(\"SELL\")\n",
    "        else:\n",
    "            signals.append(\"HOLD\")\n",
    "    return signals\n",
    "\n",
    "\n",
    "def backtest(signals, price_data, index_dates, ticker, tax_rate=0.25, trading_fee=5.0):\n",
    "    \"\"\"\n",
    "    Simulated backtesting function with correct tax on capital gains/losses.\n",
    "    \n",
    "    - Tax is applied only on the realized profit (Sell Price - Buy Price).\n",
    "    - Losses generate a negative tax credit, which offsets future gains.\n",
    "    - Trading fees are deducted on both buy and sell transactions.\n",
    "    \n",
    "    Args:\n",
    "        signals (list): List of \"BUY\"/\"SELL\" signals.\n",
    "        price_data (np.array): Corresponding stock prices.\n",
    "        index_dates (pd.Index): Date index for backtest period.\n",
    "        tax_rate (float): Tax rate applied on capital gains.\n",
    "        trading_fee (float): Flat fee per trade.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Performance metrics including tax adjustments and portfolio value.\n",
    "    \"\"\"\n",
    "\n",
    "    initial_cash = 10000\n",
    "    cash = initial_cash\n",
    "    holdings = 0\n",
    "    total_fees = 0\n",
    "    total_taxes = 0\n",
    "    tax_credit = 0  # Track losses to offset future gains\n",
    "    cost_basis = []  # Store purchase prices for accurate profit calculation\n",
    "\n",
    "    # Ensure valid index_dates\n",
    "    index_dates = pd.Series(index_dates).dropna().reset_index(drop=True)\n",
    "\n",
    "    if index_dates.empty:\n",
    "        print(\"⚠️ Warning: No valid dates available for backtest.\")\n",
    "        return None  # Exit if no valid dates\n",
    "\n",
    "    max_idx = min(len(signals), len(price_data), len(index_dates))\n",
    "\n",
    "    for i in range(max_idx):\n",
    "        price = float(price_data[i])  # Ensure price is a float\n",
    "\n",
    "        if signals[i] == \"BUY\" and cash > (price + trading_fee):\n",
    "            holdings += 1\n",
    "            cash -= (price + trading_fee)\n",
    "            total_fees += trading_fee\n",
    "            cost_basis.append(price)  # Store buy price\n",
    "\n",
    "        elif signals[i] == \"SELL\" and holdings > 0:\n",
    "            buy_price = cost_basis.pop(0)  # Get first buy price (FIFO method)\n",
    "            profit = price - buy_price  # Capital gain/loss\n",
    "\n",
    "            # Calculate tax only on profit\n",
    "            tax = profit * tax_rate\n",
    "\n",
    "            # If loss, store negative tax as credit\n",
    "            if tax < 0:\n",
    "                tax_credit += abs(tax)  # Store as a positive offset for future gains\n",
    "                tax = 0  # No tax charged on losses\n",
    "            else:\n",
    "                if tax_credit > 0:\n",
    "                    offset = min(tax, tax_credit)  # Use available credit\n",
    "                    tax -= offset\n",
    "                    tax_credit -= offset\n",
    "\n",
    "            cash += (price - tax - trading_fee)  # Add net sale amount\n",
    "            holdings -= 1\n",
    "            total_fees += trading_fee\n",
    "            total_taxes += tax\n",
    "\n",
    "    # Calculate final portfolio value\n",
    "    final_value = cash + (holdings * price_data[-1])  # Use last price for remaining stocks\n",
    "\n",
    "    # Extract backtest period\n",
    "    start_date = index_dates.iloc[0] if len(index_dates) > 0 else \"N/A\"\n",
    "    end_date = index_dates.iloc[-1] if len(index_dates) > 0 else \"N/A\"\n",
    "\n",
    "    # Calculate time delta safely\n",
    "    time_delta = (end_date - start_date).days if isinstance(start_date, pd.Timestamp) and isinstance(end_date, pd.Timestamp) else \"N/A\"\n",
    "\n",
    "    final_value = round(cash + (holdings * price_data[-1]), 2)\n",
    "    revanue = (final_value - initial_cash) / time_delta\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"Ticker\" : ticker,\n",
    "        \"Final Portfolio Value\": final_value,\n",
    "        \"Cash Remaining\": round(cash, 2),\n",
    "        \"Holdings\": holdings,\n",
    "        \"Total Fees Paid\": round(total_fees, 2),\n",
    "        \"Total Taxes Paid\": round(total_taxes, 2),\n",
    "        \"Backtest Period\": f\"{start_date} → {end_date}\",\n",
    "        \"Start Date\": start_date,\n",
    "        \"End Date\": end_date,\n",
    "        \"Time Delta (Days)\": time_delta,\n",
    "        \"yearly upside\" : revanue*365\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ==================== Section 6: Putting It All Together ====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main entry point for training and inference pipeline.\n",
    "    Reference: Specification 6.1 & 6.2 for Deployment and Real-time integration.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------- Step 1: Data Ingestion ------------------------\n",
    "    tickers = ['IWM', 'NVDA', 'QQQ', 'SPY', 'TSLA']  \n",
    "    dataset = StockDataset('day', transform=True)\n",
    "\n",
    "    # --------------------- Step 2: Data Preprocessing -----------------------\n",
    "    scaled_data, scaler, labels = preprocess_data(dataset)\n",
    "\n",
    "    # -------------------- Step 3: Train the VAE for Latent Features ---------\n",
    "    vae = VAE(input_dim=5, latent_dim=2)\n",
    "    train_vae(model=vae, data=scaled_data, epochs=5, batch_size=32, lr=1e-3)\n",
    "\n",
    "    # Obtain latent features\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        latent_list = []\n",
    "        for row in scaled_data:\n",
    "            row_torch = torch.tensor(row, dtype=torch.float32).unsqueeze(0)\n",
    "            mu, logvar = vae.encode(row_torch)\n",
    "            z = vae.reparameterize(mu, logvar)\n",
    "            latent_list.append(z.numpy().squeeze())\n",
    "    latent_features = np.array(latent_list)\n",
    "\n",
    "    # --------------------- Step 4: Train the GMM on Latent ------------------\n",
    "    gmm = train_gmm(latent_features, n_components=3)\n",
    "\n",
    "    # Example assumption: cluster 0 is bullish, 1 is bearish, 2 is neutral\n",
    "    probabilities = gmm.predict_proba(latent_features)\n",
    "    signals = generate_signals(probabilities, bullish_index=0, threshold=0.7)\n",
    "\n",
    "    # ------------------------ Step 5: Backtesting ---------------------------\n",
    "    # For demonstration, we’ll just use the Close price from the dataset\n",
    "    # Note: Ensure indices match properly with your data alignment\n",
    "    # randomaize testing\n",
    "    ticker = choice(tickers)\n",
    "    min_length = min(len(signals), len(dataset.dfs[ticker][\"Close\"].values))\n",
    "    price_data = dataset.dfs[ticker][\"Close\"].values[:min_length]\n",
    "    signals = signals[:min_length]  # Trim signals to match price_data length\n",
    "    index_dates = index_dates = dataset.dfs[ticker].index[:min_length]  # Extract corresponding dates\n",
    "\n",
    "\n",
    "    performance = backtest(signals, price_data, index_dates, ticker)\n",
    "    print(\"Backtesting Performance:\")\n",
    "    for k, v in performance.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "\n",
    "    # --------------------- Step 6: Real-time / Next Steps -------------------\n",
    "    # Integration with a live pipeline would require:\n",
    "    #   1. Fetching new data in real-time\n",
    "    #   2. Preprocessing with the same scaler\n",
    "    #   3. Getting latent features from the VAE\n",
    "    #   4. Predicting cluster probabilities via GMM\n",
    "    #   5. Generating signals for each new data point\n",
    "    print(\"\\nReal-time integration would involve repeating these steps with latest data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 85498.3376\n",
      "Epoch [2/5], Loss: 63142.3455\n",
      "Epoch [3/5], Loss: 61824.3766\n",
      "Epoch [4/5], Loss: 60982.3312\n",
      "Epoch [5/5], Loss: 60639.7539\n",
      "Backtesting Performance:\n",
      "Ticker: QQQ\n",
      "Final Portfolio Value: 83108.14\n",
      "Cash Remaining: 1066.22\n",
      "Holdings: 157\n",
      "Total Fees Paid: 2885.0\n",
      "Total Taxes Paid: 143.5\n",
      "Backtest Period: 1999-03-10 00:00:00-05:00 → 2024-12-27 00:00:00-05:00\n",
      "Start Date: 1999-03-10 00:00:00-05:00\n",
      "End Date: 2024-12-27 00:00:00-05:00\n",
      "Time Delta (Days): 9424\n",
      "yearly upside: 2831.544047113752\n",
      "\n",
      "Real-time integration would involve repeating these steps with latest data.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
